import pytest
import logging
from unittest.mock import MagicMock, AsyncMock, patch
from typer.testing import CliRunner
from contextlib import ExitStack

# Imports rely on the structure defined in conftest.py
try:
    from ingenious.services.azure_search.cli import app, setup_logging, _run_search_pipeline
    from ingenious.services.azure_search.config import SearchConfig, DEFAULT_DAT_PROMPT
    CLI_MODULE_PATH = "ingenious.services.azure_search.cli"
except ImportError:
    pytest.skip("Could not import CLI module", allow_module_level=True)


runner = CliRunner()

# --- Test setup_logging ---

def test_setup_logging_verbose():
    """Test that verbose logging sets levels to DEBUG."""
    # We patch getLogger to inspect the calls made to setLevel
    with patch("logging.getLogger") as mock_get_logger:
        mock_logger = MagicMock()
        mock_get_logger.return_value = mock_logger
        
        setup_logging(verbose=True)

        # Check that specific service loggers were targeted
        mock_get_logger.assert_any_call("ingenious.services.azure_search.pipeline")
        
        # Check that the root logger (called with no args) was also targeted
        mock_get_logger.assert_any_call()

        # Verify that setLevel(DEBUG) was called at least once (specifically for the root logger at the end)
        mock_logger.setLevel.assert_called_with(logging.DEBUG)
        
        # Verify all calls included DEBUG (as verbose sets everything to DEBUG)
        for call in mock_logger.setLevel.call_args_list:
            assert call.args[0] == logging.DEBUG

def test_setup_logging_non_verbose():
    """Test that non-verbose logging sets levels to INFO."""
    with patch("logging.getLogger") as mock_get_logger:
        mock_logger = MagicMock()
        mock_get_logger.return_value = mock_logger
        
        setup_logging(verbose=False)

        # Check that specific service loggers were targeted
        mock_get_logger.assert_any_call("ingenious.services.azure_search.pipeline")

        # Verify that setLevel(INFO) was called
        mock_logger.setLevel.assert_called_with(logging.INFO)
        
        # Verify all calls included INFO
        for call in mock_logger.setLevel.call_args_list:
            assert call.args[0] == logging.INFO

def test_setup_logging_exception_handling():
    """Test that exceptions during logger setup (e.g., permission issues) are handled."""
    with patch("logging.getLogger") as mock_get_logger:
        # Simulate a logger that fails when setLevel is called
        mock_failing_logger = MagicMock()
        mock_failing_logger.setLevel.side_effect = Exception("Config failed")
        mock_get_logger.return_value = mock_failing_logger
        
        # Should not raise an exception
        setup_logging(verbose=False)
        
        # Verify it attempted to set the level multiple times (for each logger in the list)
        assert mock_failing_logger.setLevel.call_count > 1

# --- Test _run_search_pipeline (Helper function managing asyncio) ---

# Helper to patch build_search_pipeline robustly in the CLI module
def patch_build_pipeline(mock_instance):
    return [
        patch(f'{CLI_MODULE_PATH}.build_search_pipeline', mock_instance),
    ]

# We use capsys here to capture the output generated by the Rich Console
def test_run_search_pipeline_success(config: SearchConfig, capsys):
    """Test successful execution of the pipeline runner helper."""
    query = "test successful run"

    # Mock the pipeline instance and its async methods
    mock_pipeline_instance = MagicMock()
    mock_pipeline_instance.get_answer = AsyncMock(return_value={
        "answer": "The E2E answer.",
        "source_chunks": [
            {"id": "S1", config.content_field: "Source content " * 50, "_final_score": 3.51234, "_retrieval_type": "hybrid"}
        ]
    })
    mock_pipeline_instance.close = AsyncMock()

    mock_build_pipeline = MagicMock(return_value=mock_pipeline_instance)

    # Apply patches using ExitStack for robustness
    with ExitStack() as stack:
        for p in patch_build_pipeline(mock_build_pipeline):
             stack.enter_context(p)

        # Run the helper function (it manages its own asyncio.run)
        _run_search_pipeline(config, query, verbose=False)

    # Verify the flow
    mock_build_pipeline.assert_called_once_with(config)
    mock_pipeline_instance.get_answer.assert_called_once_with(query)

    # Verify the output captured by capsys
    captured = capsys.readouterr()
    # We check for key phrases in the output (stdout/stderr depending on how Rich renders)
    output = captured.out + captured.err
    
    assert "Executing Advanced Search Pipeline" in output
    assert "The E2E answer." in output
    assert "Sources Used (1):" in output
    # Check score formatting (4 decimal places)
    assert "Score: 3.5123" in output
    assert "Type: hybrid" in output
    # Check content truncation (250 chars + ...)
    assert len("Source content " * 50) > 250
    assert ("Source content " * 50)[:250] + "..." in output

    # Verify close was called
    mock_pipeline_instance.close.assert_called_once()


def test_run_search_pipeline_configuration_error(config: SearchConfig, capsys):
    """Test handling of ValueError during pipeline construction (factory validation)."""

    # Simulate the factory raising a ValueError
    error_msg = "Semantic config missing"
    mock_build_pipeline = MagicMock(side_effect=ValueError(error_msg))

    with ExitStack() as stack:
        for p in patch_build_pipeline(mock_build_pipeline):
             stack.enter_context(p)

        _run_search_pipeline(config, "query", verbose=False)

    # Verify error output
    captured = capsys.readouterr()
    assert "Error" in captured.out
    assert f"Configuration failed: {error_msg}" in captured.out


def test_run_search_pipeline_execution_error(config: SearchConfig, capsys):
    """Test handling of generic Exception during pipeline execution."""

    # Mock the pipeline to raise an error during get_answer
    mock_pipeline_instance = MagicMock()
    mock_pipeline_instance.get_answer = AsyncMock(side_effect=RuntimeError("L1 failed"))
    mock_pipeline_instance.close = AsyncMock()

    mock_build_pipeline = MagicMock(return_value=mock_pipeline_instance)

    with ExitStack() as stack:
        for p in patch_build_pipeline(mock_build_pipeline):
            stack.enter_context(p)

        # We patch the console specifically to check if print_exception is called
        with patch(f'{CLI_MODULE_PATH}.console.print_exception') as mock_print_exception:
             _run_search_pipeline(config, "query", verbose=False)

    # Verify error output
    captured = capsys.readouterr()
    assert "Error" in captured.out
    assert "Pipeline execution failed: L1 failed" in captured.out
    assert "Run with --verbose for details." in captured.out
    
    # Verify traceback is NOT printed when verbose=False
    mock_print_exception.assert_not_called()

    # Verify close is still called in the finally block
    mock_pipeline_instance.close.assert_called_once()

def test_run_search_pipeline_execution_error_verbose(config: SearchConfig, capsys):
    """Test handling of Exception during pipeline execution with verbose=True."""
    mock_pipeline_instance = MagicMock()
    mock_pipeline_instance.get_answer = AsyncMock(side_effect=RuntimeError("L1 failed"))
    mock_pipeline_instance.close = AsyncMock()
    mock_build_pipeline = MagicMock(return_value=mock_pipeline_instance)

    with ExitStack() as stack:
        for p in patch_build_pipeline(mock_build_pipeline):
            stack.enter_context(p)

        # We patch the console specifically to check if print_exception is called
        with patch(f'{CLI_MODULE_PATH}.console.print_exception') as mock_print_exception:
             _run_search_pipeline(config, "query", verbose=True)

    # Verify traceback IS printed when verbose=True
    mock_print_exception.assert_called_once_with(show_locals=True)
    mock_pipeline_instance.close.assert_called_once()

# --- Test CLI Command (run_search) ---

# Define environment variables for CLI testing (Typer maps options to these)
CLI_ENV_VARS = {
    "AZURE_SEARCH_ENDPOINT": "https://cli-search.net",
    "AZURE_SEARCH_KEY": "searchkey123",
    "AZURE_SEARCH_INDEX_NAME": "cli-index",
    "AZURE_OPENAI_ENDPOINT": "https://cli-openai.com",
    "AZURE_OPENAI_KEY": "openaikey456",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT": "cli-embed",
    "AZURE_OPENAI_GENERATION_DEPLOYMENT": "cli-gen",
    "AZURE_SEARCH_SEMANTIC_CONFIG": "cli-semantic"
}

# Helper to patch the internal runner function
def patch_run_pipeline(mock_instance):
    return patch(f'{CLI_MODULE_PATH}._run_search_pipeline', mock_instance)

def patch_setup_logging(mock_instance):
    return patch(f'{CLI_MODULE_PATH}.setup_logging', mock_instance)


def test_cli_run_search_success():
    """Test the Typer command execution using environment variables."""
    mock_run_pipeline = MagicMock()
    mock_setup_logging = MagicMock()
    query = "test cli query"

    with patch_run_pipeline(mock_run_pipeline), patch_setup_logging(mock_setup_logging):
        # Execute the command using environment variables
        result = runner.invoke(app, [query], env=CLI_ENV_VARS)

    assert result.exit_code == 0
    assert f":mag: Starting search for: '[bold]{query}[/bold]'" in result.stdout

    # Verify logging setup (default verbose=False)
    mock_setup_logging.assert_called_once_with(False)

    # Verify the internal runner function call
    mock_run_pipeline.assert_called_once()
    args, kwargs = mock_run_pipeline.call_args
    config_arg = args[0]
    query_arg = args[1]
    verbose_arg = args[2]

    assert isinstance(config_arg, SearchConfig)
    assert query_arg == query
    assert verbose_arg is False

    # Verify configuration object construction
    assert config_arg.search_endpoint == CLI_ENV_VARS["AZURE_SEARCH_ENDPOINT"]
    assert config_arg.search_key.get_secret_value() == CLI_ENV_VARS["AZURE_SEARCH_KEY"]
    assert config_arg.semantic_configuration_name == CLI_ENV_VARS["AZURE_SEARCH_SEMANTIC_CONFIG"]
    assert config_arg.dat_prompt == DEFAULT_DAT_PROMPT


def test_cli_run_search_options_override():
    """Test that CLI options override environment variables and defaults."""
    mock_run_pipeline = MagicMock()
    mock_setup_logging = MagicMock()
    
    options = [
        "--top-k-retrieval", "50",
        "--top-n-final", "10",
        "--no-semantic-ranking",
        "--verbose",
        "--search-endpoint", "https://override.com"
    ]

    with patch_run_pipeline(mock_run_pipeline), patch_setup_logging(mock_setup_logging):
        result = runner.invoke(app, ["query"] + options, env=CLI_ENV_VARS)

    assert result.exit_code == 0
    
    # Verify logging setup (verbose=True)
    mock_setup_logging.assert_called_once_with(True)

    # Inspect configuration
    config_arg = mock_run_pipeline.call_args[0][0]
    assert config_arg.top_k_retrieval == 50
    assert config_arg.top_n_final == 10
    assert config_arg.use_semantic_ranking is False
    assert config_arg.search_endpoint == "https://override.com"


def test_cli_run_search_custom_dat_prompt_success(tmp_path):
    """Test loading a custom DAT prompt file successfully."""
    custom_prompt = "Custom DAT prompt content."
    prompt_file = tmp_path / "custom_dat.txt"
    prompt_file.write_text(custom_prompt)

    options = ["--dat-prompt-file", str(prompt_file)]
    mock_run_pipeline = MagicMock()

    with patch_run_pipeline(mock_run_pipeline):
        result = runner.invoke(app, ["query"] + options, env=CLI_ENV_VARS)

    assert result.exit_code == 0

    # Verify the configuration object has the custom prompt
    config_arg = mock_run_pipeline.call_args[0][0]
    assert config_arg.dat_prompt == custom_prompt


def test_cli_run_search_custom_dat_prompt_not_found():
    """Test error handling when the custom DAT prompt file is not found."""
    options = ["--dat-prompt-file", "non_existent_file.txt"]
    mock_run_pipeline = MagicMock()

    with patch_run_pipeline(mock_run_pipeline):
        result = runner.invoke(app, ["query"] + options, env=CLI_ENV_VARS)

    # Should exit with error code 1 (typer.Exit(code=1))
    assert result.exit_code == 1
    assert "Error: DAT prompt file not found: non_existent_file.txt" in result.stdout
    mock_run_pipeline.assert_not_called()